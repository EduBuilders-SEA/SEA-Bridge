services:
  # Next.js SEA-Bridge App
  sea-bridge-app:
    build:
      context: .
      dockerfile: Dockerfile
    expose:
      - '3000' # only exposed to internal network, Caddy will proxy from 80/443
    env_file:
      - .env
    environment:
      # Override specific values for container environment
      - NODE_ENV=production
      - OLLAMA_ENDPOINT=http://ollama:11434
    depends_on:
      - ollama
    networks:
      - sea-bridge-network
    restart: unless-stopped

  # Caddy reverse proxy with automatic HTTPS
  caddy:
    image: caddy:latest
    ports:
      - '80:80'
      - '443:443'
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    environment:
      - DOMAIN=${DOMAIN}
    depends_on:
      - sea-bridge-app
    networks:
      - sea-bridge-network
    restart: unless-stopped

  # Ollama SEA-LION Service
  ollama:
    image: ollama/ollama:latest
    expose:
      - '11434'
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    # Enable GPU access in Compose (non-Swarm)
    gpus: all
    networks:
      - sea-bridge-network
    restart: unless-stopped

volumes:
  ollama_data:
  caddy_data:
  caddy_config:

networks:
  sea-bridge-network:
    driver: bridge
